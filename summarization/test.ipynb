{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import flax.traverse_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"_name_or_path\": \"facebook/bart-base\",\n",
       "  \"activation_dropout\": 0.1,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 12,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 12,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"forced_bos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\",\n",
       "    \"7\": \"LABEL_7\",\n",
       "    \"8\": \"LABEL_8\",\n",
       "    \"9\": \"LABEL_9\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6,\n",
       "    \"LABEL_7\": 7,\n",
       "    \"LABEL_8\": 8,\n",
       "    \"LABEL_9\": 9\n",
       "  },\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"scale_embedding\": false,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 128,\n",
       "      \"min_length\": 12,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_cnn\": {\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 142,\n",
       "      \"min_length\": 56,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_xsum\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 62,\n",
       "      \"min_length\": 11,\n",
       "      \"num_beams\": 6\n",
       "    }\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.36.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.AutoConfig.from_pretrained(\"facebook/bart-base\", num_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FlaxBartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: {('final_logits_bias',)}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of FlaxBartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: {('final_logits_bias',)}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = transformers.AutoConfig.from_pretrained(\"facebook/bart-base\")\n",
    "model = transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing FlaxBertForSequenceClassification: {('cls', 'predictions', 'transform', 'dense', 'bias'), ('cls', 'predictions', 'transform', 'dense', 'kernel'), ('cls', 'predictions', 'bias'), ('cls', 'predictions', 'transform', 'LayerNorm', 'bias'), ('cls', 'predictions', 'transform', 'LayerNorm', 'scale')}\n",
      "- This IS expected if you are initializing FlaxBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: {('bert', 'pooler', 'dense', 'kernel'), ('classifier', 'kernel'), ('bert', 'pooler', 'dense', 'bias'), ('classifier', 'bias')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = transformers.AutoConfig.from_pretrained(\"bert-base-cased\", finetune_task_name=\"stsb\")\n",
    "model = transformers.FlaxAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert/embeddings/LayerNorm/bias': (768,),\n",
       " 'bert/embeddings/LayerNorm/scale': (768,),\n",
       " 'bert/embeddings/position_embeddings/embedding': (512, 768),\n",
       " 'bert/embeddings/token_type_embeddings/embedding': (2, 768),\n",
       " 'bert/embeddings/word_embeddings/embedding': (28996, 768),\n",
       " 'bert/encoder/layer/0/attention/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/0/attention/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/0/attention/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/0/attention/output/dense/kernel': (768, 768),\n",
       " 'bert/encoder/layer/0/attention/self/key/bias': (768,),\n",
       " 'bert/encoder/layer/0/attention/self/key/kernel': (768, 768),\n",
       " 'bert/encoder/layer/0/attention/self/query/bias': (768,),\n",
       " 'bert/encoder/layer/0/attention/self/query/kernel': (768, 768),\n",
       " 'bert/encoder/layer/0/attention/self/value/bias': (768,),\n",
       " 'bert/encoder/layer/0/attention/self/value/kernel': (768, 768),\n",
       " 'bert/encoder/layer/0/intermediate/dense/bias': (3072,),\n",
       " 'bert/encoder/layer/0/intermediate/dense/kernel': (768, 3072),\n",
       " 'bert/encoder/layer/0/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/0/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/0/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/0/output/dense/kernel': (3072, 768),\n",
       " 'bert/encoder/layer/1/attention/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/1/attention/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/1/attention/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/1/attention/output/dense/kernel': (768, 768),\n",
       " 'bert/encoder/layer/1/attention/self/key/bias': (768,),\n",
       " 'bert/encoder/layer/1/attention/self/key/kernel': (768, 768),\n",
       " 'bert/encoder/layer/1/attention/self/query/bias': (768,),\n",
       " 'bert/encoder/layer/1/attention/self/query/kernel': (768, 768),\n",
       " 'bert/encoder/layer/1/attention/self/value/bias': (768,),\n",
       " 'bert/encoder/layer/1/attention/self/value/kernel': (768, 768),\n",
       " 'bert/encoder/layer/1/intermediate/dense/bias': (3072,),\n",
       " 'bert/encoder/layer/1/intermediate/dense/kernel': (768, 3072),\n",
       " 'bert/encoder/layer/1/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/1/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/1/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/1/output/dense/kernel': (3072, 768),\n",
       " 'bert/encoder/layer/10/attention/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/10/attention/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/10/attention/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/10/attention/output/dense/kernel': (768, 768),\n",
       " 'bert/encoder/layer/10/attention/self/key/bias': (768,),\n",
       " 'bert/encoder/layer/10/attention/self/key/kernel': (768, 768),\n",
       " 'bert/encoder/layer/10/attention/self/query/bias': (768,),\n",
       " 'bert/encoder/layer/10/attention/self/query/kernel': (768, 768),\n",
       " 'bert/encoder/layer/10/attention/self/value/bias': (768,),\n",
       " 'bert/encoder/layer/10/attention/self/value/kernel': (768, 768),\n",
       " 'bert/encoder/layer/10/intermediate/dense/bias': (3072,),\n",
       " 'bert/encoder/layer/10/intermediate/dense/kernel': (768, 3072),\n",
       " 'bert/encoder/layer/10/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/10/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/10/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/10/output/dense/kernel': (3072, 768),\n",
       " 'bert/encoder/layer/11/attention/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/11/attention/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/11/attention/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/11/attention/output/dense/kernel': (768, 768),\n",
       " 'bert/encoder/layer/11/attention/self/key/bias': (768,),\n",
       " 'bert/encoder/layer/11/attention/self/key/kernel': (768, 768),\n",
       " 'bert/encoder/layer/11/attention/self/query/bias': (768,),\n",
       " 'bert/encoder/layer/11/attention/self/query/kernel': (768, 768),\n",
       " 'bert/encoder/layer/11/attention/self/value/bias': (768,),\n",
       " 'bert/encoder/layer/11/attention/self/value/kernel': (768, 768),\n",
       " 'bert/encoder/layer/11/intermediate/dense/bias': (3072,),\n",
       " 'bert/encoder/layer/11/intermediate/dense/kernel': (768, 3072),\n",
       " 'bert/encoder/layer/11/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/11/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/11/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/11/output/dense/kernel': (3072, 768),\n",
       " 'bert/encoder/layer/2/attention/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/2/attention/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/2/attention/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/2/attention/output/dense/kernel': (768, 768),\n",
       " 'bert/encoder/layer/2/attention/self/key/bias': (768,),\n",
       " 'bert/encoder/layer/2/attention/self/key/kernel': (768, 768),\n",
       " 'bert/encoder/layer/2/attention/self/query/bias': (768,),\n",
       " 'bert/encoder/layer/2/attention/self/query/kernel': (768, 768),\n",
       " 'bert/encoder/layer/2/attention/self/value/bias': (768,),\n",
       " 'bert/encoder/layer/2/attention/self/value/kernel': (768, 768),\n",
       " 'bert/encoder/layer/2/intermediate/dense/bias': (3072,),\n",
       " 'bert/encoder/layer/2/intermediate/dense/kernel': (768, 3072),\n",
       " 'bert/encoder/layer/2/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/2/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/2/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/2/output/dense/kernel': (3072, 768),\n",
       " 'bert/encoder/layer/3/attention/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/3/attention/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/3/attention/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/3/attention/output/dense/kernel': (768, 768),\n",
       " 'bert/encoder/layer/3/attention/self/key/bias': (768,),\n",
       " 'bert/encoder/layer/3/attention/self/key/kernel': (768, 768),\n",
       " 'bert/encoder/layer/3/attention/self/query/bias': (768,),\n",
       " 'bert/encoder/layer/3/attention/self/query/kernel': (768, 768),\n",
       " 'bert/encoder/layer/3/attention/self/value/bias': (768,),\n",
       " 'bert/encoder/layer/3/attention/self/value/kernel': (768, 768),\n",
       " 'bert/encoder/layer/3/intermediate/dense/bias': (3072,),\n",
       " 'bert/encoder/layer/3/intermediate/dense/kernel': (768, 3072),\n",
       " 'bert/encoder/layer/3/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/3/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/3/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/3/output/dense/kernel': (3072, 768),\n",
       " 'bert/encoder/layer/4/attention/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/4/attention/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/4/attention/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/4/attention/output/dense/kernel': (768, 768),\n",
       " 'bert/encoder/layer/4/attention/self/key/bias': (768,),\n",
       " 'bert/encoder/layer/4/attention/self/key/kernel': (768, 768),\n",
       " 'bert/encoder/layer/4/attention/self/query/bias': (768,),\n",
       " 'bert/encoder/layer/4/attention/self/query/kernel': (768, 768),\n",
       " 'bert/encoder/layer/4/attention/self/value/bias': (768,),\n",
       " 'bert/encoder/layer/4/attention/self/value/kernel': (768, 768),\n",
       " 'bert/encoder/layer/4/intermediate/dense/bias': (3072,),\n",
       " 'bert/encoder/layer/4/intermediate/dense/kernel': (768, 3072),\n",
       " 'bert/encoder/layer/4/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/4/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/4/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/4/output/dense/kernel': (3072, 768),\n",
       " 'bert/encoder/layer/5/attention/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/5/attention/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/5/attention/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/5/attention/output/dense/kernel': (768, 768),\n",
       " 'bert/encoder/layer/5/attention/self/key/bias': (768,),\n",
       " 'bert/encoder/layer/5/attention/self/key/kernel': (768, 768),\n",
       " 'bert/encoder/layer/5/attention/self/query/bias': (768,),\n",
       " 'bert/encoder/layer/5/attention/self/query/kernel': (768, 768),\n",
       " 'bert/encoder/layer/5/attention/self/value/bias': (768,),\n",
       " 'bert/encoder/layer/5/attention/self/value/kernel': (768, 768),\n",
       " 'bert/encoder/layer/5/intermediate/dense/bias': (3072,),\n",
       " 'bert/encoder/layer/5/intermediate/dense/kernel': (768, 3072),\n",
       " 'bert/encoder/layer/5/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/5/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/5/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/5/output/dense/kernel': (3072, 768),\n",
       " 'bert/encoder/layer/6/attention/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/6/attention/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/6/attention/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/6/attention/output/dense/kernel': (768, 768),\n",
       " 'bert/encoder/layer/6/attention/self/key/bias': (768,),\n",
       " 'bert/encoder/layer/6/attention/self/key/kernel': (768, 768),\n",
       " 'bert/encoder/layer/6/attention/self/query/bias': (768,),\n",
       " 'bert/encoder/layer/6/attention/self/query/kernel': (768, 768),\n",
       " 'bert/encoder/layer/6/attention/self/value/bias': (768,),\n",
       " 'bert/encoder/layer/6/attention/self/value/kernel': (768, 768),\n",
       " 'bert/encoder/layer/6/intermediate/dense/bias': (3072,),\n",
       " 'bert/encoder/layer/6/intermediate/dense/kernel': (768, 3072),\n",
       " 'bert/encoder/layer/6/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/6/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/6/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/6/output/dense/kernel': (3072, 768),\n",
       " 'bert/encoder/layer/7/attention/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/7/attention/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/7/attention/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/7/attention/output/dense/kernel': (768, 768),\n",
       " 'bert/encoder/layer/7/attention/self/key/bias': (768,),\n",
       " 'bert/encoder/layer/7/attention/self/key/kernel': (768, 768),\n",
       " 'bert/encoder/layer/7/attention/self/query/bias': (768,),\n",
       " 'bert/encoder/layer/7/attention/self/query/kernel': (768, 768),\n",
       " 'bert/encoder/layer/7/attention/self/value/bias': (768,),\n",
       " 'bert/encoder/layer/7/attention/self/value/kernel': (768, 768),\n",
       " 'bert/encoder/layer/7/intermediate/dense/bias': (3072,),\n",
       " 'bert/encoder/layer/7/intermediate/dense/kernel': (768, 3072),\n",
       " 'bert/encoder/layer/7/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/7/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/7/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/7/output/dense/kernel': (3072, 768),\n",
       " 'bert/encoder/layer/8/attention/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/8/attention/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/8/attention/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/8/attention/output/dense/kernel': (768, 768),\n",
       " 'bert/encoder/layer/8/attention/self/key/bias': (768,),\n",
       " 'bert/encoder/layer/8/attention/self/key/kernel': (768, 768),\n",
       " 'bert/encoder/layer/8/attention/self/query/bias': (768,),\n",
       " 'bert/encoder/layer/8/attention/self/query/kernel': (768, 768),\n",
       " 'bert/encoder/layer/8/attention/self/value/bias': (768,),\n",
       " 'bert/encoder/layer/8/attention/self/value/kernel': (768, 768),\n",
       " 'bert/encoder/layer/8/intermediate/dense/bias': (3072,),\n",
       " 'bert/encoder/layer/8/intermediate/dense/kernel': (768, 3072),\n",
       " 'bert/encoder/layer/8/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/8/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/8/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/8/output/dense/kernel': (3072, 768),\n",
       " 'bert/encoder/layer/9/attention/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/9/attention/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/9/attention/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/9/attention/output/dense/kernel': (768, 768),\n",
       " 'bert/encoder/layer/9/attention/self/key/bias': (768,),\n",
       " 'bert/encoder/layer/9/attention/self/key/kernel': (768, 768),\n",
       " 'bert/encoder/layer/9/attention/self/query/bias': (768,),\n",
       " 'bert/encoder/layer/9/attention/self/query/kernel': (768, 768),\n",
       " 'bert/encoder/layer/9/attention/self/value/bias': (768,),\n",
       " 'bert/encoder/layer/9/attention/self/value/kernel': (768, 768),\n",
       " 'bert/encoder/layer/9/intermediate/dense/bias': (3072,),\n",
       " 'bert/encoder/layer/9/intermediate/dense/kernel': (768, 3072),\n",
       " 'bert/encoder/layer/9/output/LayerNorm/bias': (768,),\n",
       " 'bert/encoder/layer/9/output/LayerNorm/scale': (768,),\n",
       " 'bert/encoder/layer/9/output/dense/bias': (768,),\n",
       " 'bert/encoder/layer/9/output/dense/kernel': (3072, 768),\n",
       " 'bert/pooler/dense/bias': (768,),\n",
       " 'bert/pooler/dense/kernel': (768, 768),\n",
       " 'classifier/bias': (2,),\n",
       " 'classifier/kernel': (768, 2)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.tree_util\n",
    "jax.tree_util.tree_map(lambda v: v.shape, flax.traverse_util.flatten_dict(model.params, sep=\"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.decoder_start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is the\n"
     ]
    }
   ],
   "source": [
    "# ARTICLE_TO_SUMMARIZE = \"summarize: My friends are cool but they eat too many carbs.\"\n",
    "ARTICLE_TO_SUMMARIZE = \"Where is the\"\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], return_tensors=\"np\")\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=10).sequences\n",
    "print(tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
